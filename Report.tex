% Header

\documentclass[conference]{IEEEtran}
\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  \graphicspath{{./report/}}
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  \usepackage[dvips]{graphicx}
  \graphicspath{{./report/}}
  \DeclareGraphicsExtensions{.jpg}
\fi

\hyphenation{op-tical net-works semi-conduc-tor}
\graphicspath{ {./report/} }

\usepackage{amsmath}

\begin{document}
\title{Music Classification - Pick a beteer title later}
\author{\IEEEauthorblockN{Christian Johnson}
  \IEEEauthorblockA{Electrical Engineering Department\\
    United States Coast Guard Academy\\
    New London, Connecticut 06320\\
    Email: Christian.S.Johnson@USCGA.edu}
  \and
  \IEEEauthorblockN{Daniel Nusraty}
  \IEEEauthorblockA{Electrical Engineering Department\\
    United States Coast Guard Academy\\
    New London, Connecticut 06320\\
    Email: Daniel.Y.Nusraty@USCGA.edu}
  \and
  \IEEEauthorblockN{Joshua West}
  \IEEEauthorblockA{Electrical Engineering Department\\
    United States Coast Guard Academy\\
    New London, Connecticut 06320\\
    Email: Joshua.C.West@USCGA.edu}}

\maketitle
\begin{abstract}
  This is an abstract
\end{abstract}

\section{Introduction}
\subsection{Motivation}
With the advent of music subscription services, many people have become accustomed to automatically generated playlist, tailor-made to their own personal taste. These services provide such playlists at the push of a button, analyzing the user's listening history to continuously recommend similar songs. Providers such as Pandora, Spotify, and LastFM maintain massive databases containing context information on millions of songs in order to present the best recommendations. For those who prefer to maintain their own local music library however, the options for tailored music recommendations are dramatically reduced. In this paper, we seek to explore the application of digital signal processing techniques in order to implement similar music recommendation functionality that will work with local music files. 
\subsection{Similar Work}
\section{Theory}
\subsection{Background}
The majority of music analysis is based on a family of functions known as Fourier Transforms. A Fourier Transform is responsible for transforming time-domain audio information into a frequency-domain representation. In the time domain, sound is represented as amplitude as a function of time. In the frequency domain, sound is instead represented as a function of magnitude based on frequency. What is important to recognize, is that in both cases the signal is still continuous, meaning unbroken. A continuous signal contains a great deal of information that makes it difficult to perform operations on. In order for a computer to be able to process such a signal, that amount of information must be reduced through an operation known as \textit{sampling}. Sampling replaces the continuous signal with discrete representation; an array of values at (typically) evenly spaced indeces of time. The normal Fourier Transform cannot operate on discrete signals however, which is where a specific type of Fourier Transform known as the Discrete Fourier Transform (DFT) comes in. The DFT is comprised of a single summation operation, which will produce an array of complex number representations.
\begin{equation}
  X(k)=\frac{1}{N}\sum_{n=0}^{N-1}x(n)e^{-j\frac{2\pi}{N}kn}\label{DFT}
\end{equation}
Each value in the DFT output $S[k]$ is a complex number that represents a point in an $N$ dimensional vector space. The magnitude and phase of that point represent the content of the time-domain signal at frequency $\frac{2k\pi}{N}$, where $N$ represents the length of $S[k]$. These values depend a great deal on a concept known as \textit{sampling frequency}, which refers to the time between each sample taken of the original time-domain signal. Each 'bin' of $S[k]$ represent a collection of frequencies, $\frac{-F_{s}}{2N} + \frac{k*F_{s}}{N} < f < \frac{F_{s}}{2N} + \frac{k*F_{s}}{N}$. As such, it is important to recognize that $S[1]$ does not directly correspond to a specific frequency value, and in order to find the DFT output for, say, 20Hz, one must determine which bin this frequency will fall into.
\subsubsection{The FFT}
In its original form, the DFT is quite computationally expensive and complex, comprised of $\mathcal{O}(n^{s})$ operations. An algorithm known as the Fast Fourier Transform (FFT) dramatically simplifies this complexity, reducing the expense to $\mathcal{O}(n*log(n))$ operations. It does this by exploiting symmetry within the DFT. Equation \eqref{DFT}, the basic representation for the DFT, is periodic about $N$, i.e. $X_{k+l*N}=X_{k}$ for any integer $l$. The FFT uses this relationship to break the DFT into even and odd components
\begin{equation}\label{FFT}
  \begin{split}
 X_{k} & =\sum_{n=0}^{N-1}{x(n)e^{-j2\pi kn/N}}\\
  & =\sum_{m=0}^{N/2-1}{x(2m)*e^{-j2\pi km/(N/2)}}\\ & + \sum_{m=0}^{N/2-1}{x(2m+1)*e^{-j2\pi km/(N/2)}}
  \end{split}
\end{equation}
This particular implementation is known as the Cooley-Tukey FFT, and is one of the most widely used algorithms in the world. 
\subsection{Analyzing Music}
In order to analyze and classify music, it is important to understand how it is structured. Fundamentally, music is simply composed of air molecules vibrating at different frequencies and amplitudes. This means that there are a theoretically infinite number of ways to structure and divide these different frequencies, but in western music, the most common method uses what are known as major scales. The major scales are composed of 7 unique notes, A through G order based on their pitch interval. While advanced music theory is outside of the scope for this paper, it is important to recognize that a single major scale is composed of 7 notes, with a predetermined order of pitch difference. These pitch differences are somewhat unintuitive however, since music is arranged on a logarithmic scale, meaning that the note typically known as A0 (The note A in the first recognized octave) is defined as 27.5 Hz, A1 is 55 Hz, A2 is 110 Hz, and so on. This is significant, because it means that the most commonly used frequencies in music will typically be grouped towards the lower half of the established musical range. In general, musical notes are defined from C0 at 16.35 Hz all the way to B8 at 7902.13 Hz. Most types of music will use notes toward the center of this range, as the extremes are considered somewhat unpleasant; however, different genres of music may use varying distributions of these frequencies. For instance, an EDM or electronic artist may choose to use frequencies in the 0 or 1 octave in order to produce a more physical feeling. These frequencies are considered ``super-low'', since they are so low that, when combined with other tones, they are more often \textit{felt} than heard.
\section{Experimental Procedure}
Using this information, it becomes relatively simple to start processing audio files and comparing them to each other. There are 3 key steps to this process; audio information should be read from a file, then that audio information should be compressed and generalized in order to compare two songs. This generalized audio information should be collated into some structure that will allow simple access later. The simplest application for this concept is to compare a collection of songs to a single different audio file and return a list of songs from the collection that are most similar to that audio file.
\subsection{Processing a Single Audio File}
Digital audio files are simply a collection of amplitude information arranged based on time. This information is sampled from the original continuous audio source, typically at a rate of 44,100 Hz (Meaning that there should be 44,100 samples making up a single seconds worth of audio information). This format is incredibly useful for a computer which must play recorded audio, since the data will simply tell a computer the \textit{amplitude} at which to vibrate a speaker diaphragm, and the time at which to do so. It is less applicable to examining and classifying the underlying composition of the audio however, and must be tranformed using the FFT to a more applicable format. Taking the FFT shifts the sampled data from amplitude in terms of time to amplitude in terms of frequency, as discussed earlier.
\subsection{Interpreting Data}
The FFT output is formatted in a much more accessible fashion than the time domain data. It provides an array of complex bin values that represent a range of frequencies as discussed earlier. In order to use this information to compare two song files, it is important to generalize the information somewhat. Comparing the raw FFT of two files can hold some value, but they contain such a high volume of information that such a comparison could result in too high a degree of specificity. One method for generalizing this data is to separate the frequency spectrum into so-called 'bands'. These bands can be chosen based on a variety of different factors, but it seems most logical to divide them based on musical structure. Based on the scale structure discussed earlier in this paper, a reasonable set of frequency breakpoints could be $0-60\text{ Hz}$ to represent 'superlow' frequencies, $61-240\text{ Hz}$ for lows, $241-500\text{ Hz}$ for mids, $501-2000\text{ Hz}$ for highs, and $2001-8000\text{ Hz}$ to represent 'superhighs'. Since the FFT bin indeces do not correspond to actual frequencies, it is necessary to convert these frequencies in order to determine the closest bins that these frequencies correspond to. Calculating the bin into which a specific frequency will fall relies upon the relationship between sampling frequency and sequence length. If $N$ represents the number of samples in an FFT signal, $f$ represents the frequency value, and $r$ represents sampling rate then bin number can be found from the following equation:
\begin{equation}\label{BinNum}
  \text{Bin Index}=\frac{f * N}{r}
\end{equation}
Using this equation, it becomes a trivial operation to determine the necessary breakpoint indeces, then to segment the FFT data into a set of bands. 


\section{Results}

\section{Bibliography}
We will move all these sources to a .bib file later and import using BibTex.
Until we figure out how to do that, they are referenced here by section.
\subsection{Similar Work}
https://www.music-tomorrow.com/blog/how-spotify-recommendation-system-works-a-complete-guide-2022
https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition
\subsection{Theory}
https://dsp.stackexchange.com/questions/5915/what-is-the-meaning-of-the-dft
https://dsp.stackexchange.com/questions/26927/what-is-a-frequency-bin
https://stackoverflow.com/questions/10754549/fft-bin-width-clarification
\subsection{FFT}
https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter24.03-Fast-Fourier-Transform.html
\subsection{Analysing Music}
https://muted.io/note-frequencies/
https://rwu.pressbooks.pub/musictheory/chapter/major-scales/
https://people.umass.edu/gmhwww/382/pdf/12-music%20scales.pdf
https://www.onlinepianocoach.com/common-music-scales.html
https://www.petervis.com/hi-fi-info/jvc-sea-graphic-equalizers/frequency-spectrum-of-common-musical-instruments.html



\end{document}

% LocalWords:  DFT WAV indeces FFT Spotify LastFM Tukey
